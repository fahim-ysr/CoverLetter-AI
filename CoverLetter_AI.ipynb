{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd28c8d3-7911-4ece-9064-da4706dc0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Dependencies:\n",
    "\n",
    "# pip install chromadb\n",
    "# pip install langchain_groq\n",
    "# pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e84289fe-68bd-4036-8161-ab0eecedefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import csv\n",
    "import uuid\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a7d7d0-06f6-4037-a2eb-19ca7bd3b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the Groq API Key from '.env.local' file\n",
    "\n",
    "import os                                                                                                                                                                                                          \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv(Path(\".env.local\"))\n",
    "KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d5464-0e3d-4e8a-8023-70ceda0375db",
   "metadata": {},
   "source": [
    "## Testing the ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "197d1ba3-4db7-4be0-9a4b-7335bdb0a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The richest man in the world of all time is a matter of debate among historians and economists, as the concept of wealth and its measurement have changed over time. However, according to various sources, including Forbes and other financial publications, the richest person in history is often considered to be:\n",
      "\n",
      "1. **Mansa Musa I** (1280-1337): The king of the Mali Empire, who ruled over a vast territory in West Africa during the 14th century. His wealth is estimated to be around $400 billion in today's dollars, making him the richest person in history.\n",
      "\n",
      "Mansa Musa's wealth came from his control of the trans-Saharan trade, which included gold, salt, and other valuable commodities. He was known for his extravagant spending and generosity, and his pilgrimage to Mecca in 1324 was said to have been so lavish that it caused a shortage of gold in Egypt.\n",
      "\n",
      "Other contenders for the title of richest person in history include:\n",
      "\n",
      "* **John D. Rockefeller** (1839-1937): The American oil industry magnate, who founded Standard Oil and was estimated to be worth around $336 billion in today's dollars.\n",
      "* **Andrew Carnegie** (1835-1919): The Scottish-American industrialist and philanthropist, who made his fortune in the steel industry and was estimated to be worth around $309 billion in today's dollars.\n",
      "* **Akbar the Great** (1542-1605): The Mughal emperor of India, who was estimated to be worth around $290 billion in today's dollars.\n",
      "* **Genghis Khan** (1162-1227): The Mongol emperor, who was estimated to be worth around $230 billion in today's dollars.\n",
      "\n",
      "Note: These estimates are based on various sources and should be used as a rough guide only, as the calculation of historical wealth is often subject to interpretation and debate.\n",
      "\n",
      " ***Preambled Answer***\n",
      "\n",
      "Mansa Musa I of the Mali Empire, with an estimated net worth of $400 billion.\n"
     ]
    }
   ],
   "source": [
    "# Testing if ChatGroq is running correctly\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model= 'llama-3.3-70b-versatile',\n",
    "    temperature= 0,\n",
    "    groq_api_key= KEY,\n",
    "    max_tokens= None,\n",
    "    timeout= None,\n",
    "    max_retries= 2,\n",
    "    # Other Parameters...\n",
    ")\n",
    "\n",
    "answer = llm.invoke(\"Who is the richest man in the world of all time?\")\n",
    "print(answer.content)\n",
    "print(\"\\n ***Preambled Answer***\\n\")\n",
    "answer = llm.invoke(\"Who is the richest man in the world of all time?, no preamble\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1803bf19-dcd2-4f24-a1b4-0057202fde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chromadb Setup\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name = \"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa838f8-71e8-4664-9117-f3d39e1abb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    \n",
    "    documents = [\n",
    "        \"I like drinking water everyday\",\n",
    "        \"I like my apple iphone\",\n",
    "        \"Do you like apple?\"\n",
    "    ],\n",
    "\n",
    "    ids = [\"id1\", \"id2\", \"id3\"],\n",
    "\n",
    "    metadatas = [\n",
    "        {\"url\": \"source_to_id1\"},\n",
    "        {\"url\": \"source_to_id2\"},\n",
    "        {\"url\": \"source_to_id3\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "923ed22f-0f2c-4cb0-9e72-0f0776cafd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['id1', 'id2', 'id3'], 'embeddings': None, 'documents': ['I like drinking water everyday', 'I like my apple iphone', 'Do you like apple?'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'url': 'source_to_id1'}, {'url': 'source_to_id2'}, {'url': 'source_to_id3'}]}\n"
     ]
    }
   ],
   "source": [
    "print(collection.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3988b4d6-c203-47c7-a834-3fe482602780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['id2'], 'embeddings': None, 'documents': ['I like my apple iphone'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'url': 'source_to_id2'}]}\n"
     ]
    }
   ],
   "source": [
    "print(collection.get(ids = [\"id2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "419acc0a-c695-4757-9c8c-d6c2ee3ee2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id2', 'id3']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['I like my apple iphone', 'Do you like apple?']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'url': 'source_to_id2'}, {'url': 'source_to_id3'}]],\n",
       " 'distances': [[1.0667046308517456, 1.2515640258789062]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_texts = [\"I like technology\"], n_results = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c07bcf4-3f44-41f9-81ae-cad8bc5101e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['I like drinking water everyday']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'url': 'source_to_id1'}]],\n",
       " 'distances': [[1.3786277770996094]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_texts = [\"Do you eat healthy?\"], n_results = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16052f41-4083-40d5-8b77-2fe3fb638cc7",
   "metadata": {},
   "source": [
    "## Extracting Data From Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e007f77-8970-4614-a41a-647c1ac98670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the WebBaseLoader to scrape content of a webpage. Using a demo job description link for testing purposes\n",
    "loader = WebBaseLoader(\"https://jobs.foundever.com/job/Canada-IT-Site-System-Supt-Admin-Onsite-in-London%2C-ON-Cana/1286688600/?\")\n",
    "\n",
    "# Stores the loaded data of the webpage\n",
    "data = loader.load().pop().page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "df508916-10c1-4dcb-aef0-a0473849d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LLM that is going to be used for data extraction\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model= 'llama-3.3-70b-versatile',\n",
    "    temperature= 0.5,\n",
    "    groq_api_key= KEY,\n",
    "    max_tokens= None,\n",
    "    timeout= None,\n",
    "    max_retries= 2,\n",
    "    # Other Parameters...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eb7a6ed8-74a1-4106-9273-83e3f3dd3537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"role\": \"Data Engineer/Scientist\",\n",
      "    \"experience\": \"Not specified\",\n",
      "    \"skills\": [\n",
      "        \"Python\",\n",
      "        \"SQL\",\n",
      "        \"Pandas\",\n",
      "        \"Airflow\",\n",
      "        \"PySpark\",\n",
      "        \"Spark SQL\",\n",
      "        \"Delta Lake\",\n",
      "        \"Machine Learning\",\n",
      "        \"Deep Learning\",\n",
      "        \"TensorFlow\",\n",
      "        \"Data Engineering\",\n",
      "        \"ETL\",\n",
      "        \"ELT\",\n",
      "        \"Cloud Platforms (AWS, GCP, Azure)\",\n",
      "        \"Data Warehousing\",\n",
      "        \"Data Modeling\",\n",
      "        \"DBT\",\n",
      "        \"Data Visualization\",\n",
      "        \"Power BI\",\n",
      "        \"Tableau\",\n",
      "        \"MLOps\",\n",
      "        \"MLflow\",\n",
      "        \"Kubeflow\",\n",
      "        \"Natural Language Processing (NLP)\",\n",
      "        \"NLTK\",\n",
      "        \"spaCy\",\n",
      "        \"Computer Vision\",\n",
      "        \"OpenCV\",\n",
      "        \"Time Series Analysis\",\n",
      "        \"Forecasting\",\n",
      "        \"Prophet\",\n",
      "        \"Data Cleaning\",\n",
      "        \"Data Wrangling\",\n",
      "        \"Feature Engineering\",\n",
      "        \"Scikit-learn\",\n",
      "        \"Statistical Analysis\",\n",
      "        \"Hypothesis Testing\",\n",
      "        \"Data Ethics\",\n",
      "        \"Privacy\",\n",
      "        \"GDPR\",\n",
      "        \"Big Data\",\n",
      "        \"Hadoop\",\n",
      "        \"Spark\",\n",
      "        \"Data Governance\",\n",
      "        \"Data Quality\",\n",
      "        \"Metadata Management\",\n",
      "        \"Data Security\",\n",
      "        \"Encryption\",\n",
      "        \"Tokenization\",\n",
      "        \"Cloud Data Engineering\",\n",
      "        \"AWS Glue\",\n",
      "        \"Databricks\"\n",
      "    ],\n",
      "    \"description\": \"The role involves working on various data engineering and science projects, including data warehousing, data modeling, data visualization, machine learning, and data governance. The ideal candidate should have experience with a range of tools and technologies, including Python, SQL, Pandas, Airflow, PySpark, and TensorFlow.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Using prompt on the model\n",
    "\n",
    "extract_from_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I will give you scraped text from job postings. Your job is to extract the details and requirements in a JSON format containing the following keywords: 'role', 'experience', 'skills', 'description'\n",
    "    Only return the JSON. No preamble please.\n",
    "    Here is the scraped text: {page_data}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain_extract = extract_from_prompt | llm\n",
    "answer = chain_extract.invoke(input= {'page_data': data})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bedd2035-46c5-4f4d-a2af-6ae669fd8fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'Data Engineer/Scientist',\n",
       " 'experience': 'Not specified',\n",
       " 'skills': ['Python',\n",
       "  'SQL',\n",
       "  'Pandas',\n",
       "  'Airflow',\n",
       "  'PySpark',\n",
       "  'Spark SQL',\n",
       "  'Delta Lake',\n",
       "  'Machine Learning',\n",
       "  'Deep Learning',\n",
       "  'TensorFlow',\n",
       "  'Data Engineering',\n",
       "  'ETL',\n",
       "  'ELT',\n",
       "  'Cloud Platforms (AWS, GCP, Azure)',\n",
       "  'Data Warehousing',\n",
       "  'Data Modeling',\n",
       "  'DBT',\n",
       "  'Data Visualization',\n",
       "  'Power BI',\n",
       "  'Tableau',\n",
       "  'MLOps',\n",
       "  'MLflow',\n",
       "  'Kubeflow',\n",
       "  'Natural Language Processing (NLP)',\n",
       "  'NLTK',\n",
       "  'spaCy',\n",
       "  'Computer Vision',\n",
       "  'OpenCV',\n",
       "  'Time Series Analysis',\n",
       "  'Forecasting',\n",
       "  'Prophet',\n",
       "  'Data Cleaning',\n",
       "  'Data Wrangling',\n",
       "  'Feature Engineering',\n",
       "  'Scikit-learn',\n",
       "  'Statistical Analysis',\n",
       "  'Hypothesis Testing',\n",
       "  'Data Ethics',\n",
       "  'Privacy',\n",
       "  'GDPR',\n",
       "  'Big Data',\n",
       "  'Hadoop',\n",
       "  'Spark',\n",
       "  'Data Governance',\n",
       "  'Data Quality',\n",
       "  'Metadata Management',\n",
       "  'Data Security',\n",
       "  'Encryption',\n",
       "  'Tokenization',\n",
       "  'Cloud Data Engineering',\n",
       "  'AWS Glue',\n",
       "  'Databricks'],\n",
       " 'description': 'The role involves working on various data engineering and science projects, including data warehousing, data modeling, data visualization, machine learning, and data governance. The ideal candidate should have experience with a range of tools and technologies, including Python, SQL, Pandas, Airflow, PySpark, and TensorFlow.'}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the output into JSON using parser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "json_answer = parser.parse(answer.content)\n",
    "json_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bf7de64c-f4e6-4bde-bb2c-f8c9fba7466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Checking the type of output for confirmation\n",
    "print(type(json_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b0b93-6933-4524-a0cb-75dec94365f7",
   "metadata": {},
   "source": [
    "#### Importing Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3d06eafc-dc03-46f4-83f0-e1feff6c3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function reads and formats the data in portfolio so that the LLM can understand\n",
    "\n",
    "def read_file(path):\n",
    "    current = []\n",
    "    with open(path, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)\n",
    "        for line in csv_reader:\n",
    "            # Formatting data\n",
    "            skills = tuple(line[:-1])\n",
    "            link = line[-1]\n",
    "            current.append((skills, link))\n",
    "        return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d57e1f94-ce98-4c6a-90ae-38ed051c896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Python', ' SQL', ' Pandas')  https://github.com/user/project1\n",
      "('SQL', ' Python', ' Airflow')  https://github.com/user/project2\n",
      "('PySpark', ' Spark SQL', ' Delta Lake')  https://github.com/user/project3\n",
      "('Machine Learning', ' Deep Learning', ' TensorFlow')  https://github.com/user/project4\n",
      "('Data Engineering', ' ETL', ' ELT')  https://github.com/user/project5\n",
      "('Cloud Platforms (AWS', ' GCP', ' Azure)')  https://github.com/user/project6\n",
      "('Data Warehousing', ' Data Modeling', ' DBT')  https://github.com/user/project7\n",
      "('Data Visualization', ' Power BI', ' Tableau')  https://github.com/user/project8\n",
      "('MLOps', ' MLflow', ' Kubeflow')  https://github.com/user/project9\n",
      "('Natural Language Processing (NLP)', ' NLTK', ' spaCy')  https://github.com/user/project10\n",
      "('Computer Vision', ' OpenCV', ' TensorFlow')  https://github.com/user/project11\n",
      "('Time Series Analysis', ' Forecasting', ' Prophet')  https://github.com/user/project12\n",
      "('Data Cleaning', ' Data Wrangling', ' Pandas')  https://github.com/user/project13\n",
      "('Feature Engineering', ' Scikit-learn')  https://github.com/user/project14\n",
      "('Statistical Analysis', ' Hypothesis Testing')  https://github.com/user/project15\n",
      "('Data Ethics', ' Privacy', ' GDPR')  https://github.com/user/project16\n",
      "('Big Data', ' Hadoop', ' Spark')  https://github.com/user/project17\n",
      "('Data Governance', ' Data Quality', ' Metadata Management')  https://github.com/user/project18\n",
      "('Data Security', ' Encryption', ' Tokenization')  https://github.com/user/project19\n",
      "('Cloud Data Engineering', ' AWS Glue', ' Databricks')  https://github.com/user/project20\n"
     ]
    }
   ],
   "source": [
    "# Sample Usage\n",
    "\n",
    "path = 'demo_portfolio.csv'\n",
    "data = read_file(path)\n",
    "\n",
    "\n",
    "for skills, link in data:\n",
    "    print(skills, link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de9949-eaaf-42ce-9d7d-82c2e852d8dd",
   "metadata": {},
   "source": [
    "#### Inserting data into Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f64c40dd-0c82-4207-b12d-fc1c23a5f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chromadb Setup\n",
    "\n",
    "client = chromadb.PersistentClient('vectorstore')\n",
    "collection = client.get_or_create_collection(name= 'portfolio_links')\n",
    "\n",
    "if not collection.count():\n",
    "    for skills, links in data:\n",
    "        collection.add(\n",
    "            documents= str(skills),\n",
    "            metadatas= {'portfolio_url': links},\n",
    "            ids= [str(uuid.uuid4())]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "68eff6be-3b7d-4042-8426-11bd7f4028f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displays the first skills required in the sample job description\n",
    "\n",
    "json_answer['skills'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6c6059f2-33a8-48fa-9d8e-794ac614fbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['e4e3821b-db9a-4430-8114-1eb7453ee5f8',\n",
       "   'ef127efc-1cf1-41ce-beab-937f122ff417']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"('SQL', ' Python', ' Airflow')\",\n",
       "   \"('Python', ' SQL', ' Pandas')\"]],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'portfolio_url': ' https://github.com/user/project2'},\n",
       "   {'portfolio_url': ' https://github.com/user/project1'}]],\n",
       " 'distances': [[1.009972095489502, 1.0451734066009521]]}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Queries the portfolio for a relevant project based on the first required skill as above\n",
    "\n",
    "urls = collection.query(query_texts= json_answer['skills'][0], n_results= 2)\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e1508-ef79-4967-a6e1-2f2c5fec7e63",
   "metadata": {},
   "source": [
    "## Setting up prompt for writing the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f8f56c77-e746-46a0-a608-e07de18f9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning variable for the dob description in JSON\n",
    "\n",
    "job_desc = json_answer['description']\n",
    "\n",
    "# Prompt for the email\n",
    "\n",
    "email_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I will give you a role and a task that you have to perform in that specific role.\n",
    "    Your role: Your name is Fahim. You are an incredible business development officer who who knows how to get clients. You work for ABC Consulting Firm, your firm works with all kinds of IT clients and provide solutions in the domain of Data Science and AI.\n",
    "    CoverLetter AI focuses on efficient tailored solutions for all clients keeping costs down. \n",
    "    Your Job: Your Job is to write cold emails to clients regarding the Job openings that they have advertised. Try to pitch your clients with an email hook that opens a conversation about a possibility of working with them. Add the most relevant portfolio URLs from\n",
    "    the following (shared below) to showcase that we have the right expertise to get the job done. \n",
    "    I will now provide you with the Job description and the portfolio URLs:\n",
    "    JOB DESCRIPTION: {job_description}\n",
    "    ------\n",
    "    PORTFOLIO URLS: {portfolio_urls}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Extracting the answer\n",
    "chain_email = email_prompt | llm\n",
    "answer = chain_email.invoke({'job_description': job_desc, 'portfolio_urls': urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b76863f2-d7f7-404d-9fb3-e3f216bd148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Expert Data Science and AI Solutions for Your Engineering and Science Projects\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I came across your job posting for a data engineering and science role, and I was impressed by the scope of the projects you're working on, including data warehousing, data modeling, data visualization, machine learning, and data governance. As a Business Development Officer at ABC Consulting Firm, I'd like to introduce you to our team of experts who can help you achieve your goals efficiently and cost-effectively.\n",
      "\n",
      "At ABC Consulting Firm, we specialize in providing tailored Data Science and AI solutions that cater to the unique needs of our clients. Our team has extensive experience working with a range of tools and technologies, including Python, SQL, Pandas, Airflow, PySpark, and TensorFlow. We're confident that our expertise aligns perfectly with your project requirements.\n",
      "\n",
      "I'd like to draw your attention to some of our notable projects that demonstrate our capabilities:\n",
      "\n",
      "* https://github.com/user/project2 (Data Engineering and Science projects using SQL, Python, and Airflow)\n",
      "* https://github.com/user/project1 (Data Science projects using Python, SQL, and Pandas)\n",
      "\n",
      "These projects showcase our ability to design, develop, and deploy scalable data engineering and science solutions that drive business value. Our team is well-versed in handling complex data projects, and we're excited about the possibility of collaborating with you.\n",
      "\n",
      "I'd love to schedule a call to discuss how our team can support your data engineering and science initiatives. Please let me know if you're available for a brief conversation this week or next.\n",
      "\n",
      "Looking forward to hearing from you and exploring how we can help you achieve your goals.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Fahim\n",
      "Business Development Officer\n",
      "ABC Consulting Firm\n",
      "CoverLetter AI - Efficient Tailored Solutions for All Clients, Keeping Costs Down.\n"
     ]
    }
   ],
   "source": [
    "# Email that has been generated\n",
    "\n",
    "email = answer.content\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3b210-15c7-447b-b208-9bb9e3c3aad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
